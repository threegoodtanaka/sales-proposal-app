"""
ãƒã‚±ãƒ‘ãƒ© å˜ä¸€åº—èˆ—ãƒ†ã‚¹ãƒˆ
"""
import sys
import re
import urllib.request

if sys.platform == "win32":
    import io
    for name in ("stdout", "stderr"):
        stream = getattr(sys, name)
        if hasattr(stream, "buffer"):
            setattr(sys, name, io.TextIOWrapper(stream.buffer, encoding="utf-8", errors="replace", line_buffering=True))

try:
    from bs4 import BeautifulSoup
    print("âœ“ BeautifulSoup4 OK")
except ImportError:
    print("âœ— BeautifulSoup4 ãŒå¿…è¦ã§ã™")
    sys.exit(1)

# ãƒ†ã‚¹ãƒˆURL
TEST_URL = "https://www.pokepara.jp/kyoto/m325/a384/shop12005/"

def fetch(url):
    """URLã‹ã‚‰HTMLã‚’å–å¾—"""
    print(f"ğŸ“¡ å–å¾—ä¸­: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en;q=0.9",
        "Referer": "https://www.pokepara.jp/",
    }
    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, timeout=15) as res:
        html = res.read()
    for enc in ("utf-8", "shift_jis", "cp932"):
        try:
            return html.decode(enc)
        except:
            continue
    return html.decode("utf-8", errors="replace")

def parse_shop(html):
    """åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
    result = {"name": "", "area_type": "", "address": "", "phone": ""}
    soup = BeautifulSoup(html, "html.parser")
    
    # åº—èˆ—å
    h1 = soup.find("h1")
    if h1:
        name = h1.get_text(strip=True)
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+/(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼)[^\n]*$', '', name)
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+$', '', name)
        result["name"] = name.strip()
    
    # åœ°åŸŸãƒ»æ¥­æ…‹
    breadcrumb = soup.find("div", class_=re.compile(r"breadcrumb", re.I))
    if breadcrumb:
        text = breadcrumb.get_text(strip=True)
        parts = [p.strip() for p in text.split('>')]
        if len(parts) >= 2:
            area = parts[-2] if len(parts) >= 2 else ""
            genre = parts[-1] if parts[-1] in ["ã‚­ãƒ£ãƒã‚¯ãƒ©", "ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼", "ãƒ©ã‚¦ãƒ³ã‚¸", "ã‚¹ãƒŠãƒƒã‚¯", "ã‚¯ãƒ©ãƒ–", "ãƒ‘ãƒ–"] else ""
            if area and genre:
                result["area_type"] = f"{area} {genre}"
    
    if not result["area_type"]:
        patterns = [
            r'(ç¥‡åœ’|æœ¨å±‹ç”º|å…ˆæ–—ç”º|æ²³åŸç”º|å››æ¡|ä¸‰æ¡|çƒä¸¸|äº¬éƒ½é§…|äºŒæ¡|è¥¿é™¢|è¥¿äº¬æ¥µ)\s*(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼|ãƒ©ã‚¦ãƒ³ã‚¸|ã‚¹ãƒŠãƒƒã‚¯|ã‚¯ãƒ©ãƒ–|ãƒ‘ãƒ–)',
        ]
        for pattern in patterns:
            m = re.search(pattern, html)
            if m:
                result["area_type"] = f"{m.group(1)} {m.group(2)}"
                break
    
    # ä½æ‰€
    addr_patterns = [
        r'ä½æ‰€[ï¼š:]\s*<[^>]+>([^<]+)<',
        r'ä½æ‰€[ï¼š:]\s*([^\n<]{10,150})',
        r'(äº¬éƒ½åºœäº¬éƒ½å¸‚[^\n<"]{10,150})',
        r'(äº¬éƒ½åºœ[^\n<"]{10,150})',
    ]
    for pattern in addr_patterns:
        m = re.search(pattern, html)
        if m:
            addr = m.group(1)
            addr = re.sub(r'<[^>]+>', '', addr)
            addr = re.sub(r'"\s*/>', '', addr)
            addr = re.sub(r'"[^"]*$', '', addr)
            addr = re.sub(r'ä½æ‰€[ï¼š:]', '', addr)
            addr = addr.strip()
            if len(addr) > 10:
                result["address"] = addr
                break
    
    # é›»è©±ç•ªå·
    phone_patterns = [
        r'(?:tel|TEL|é›»è©±)[ï¼š:]\s*(0\d{1,4}[-]\d{1,4}[-]\d{4})',
        r'(?<![0-9])(0\d{1,4}[-]\d{1,4}[-]\d{4})(?![0-9])',
    ]
    for pattern in phone_patterns:
        m = re.search(pattern, html, re.I)
        if m:
            phone = m.group(1).replace(" ", "")
            phone_digits = re.sub(r'[^0-9]', '', phone)
            if 10 <= len(phone_digits) <= 11:
                result["phone"] = phone
                break
    
    return result

print("=" * 70)
print("ğŸ° ãƒã‚±ãƒ‘ãƒ©å˜ä¸€åº—èˆ—ãƒ†ã‚¹ãƒˆ")
print("=" * 70)
print(f"URL: {TEST_URL}")
print()

try:
    html = fetch(TEST_URL)
    print(f"âœ“ HTMLå–å¾—: {len(html):,}æ–‡å­—\n")
    
    info = parse_shop(html)
    
    print("ğŸ“‹ æŠ½å‡ºçµæœ:")
    print(f"  åº—èˆ—å: {info['name']}")
    print(f"  åœ°åŸŸãƒ»æ¥­æ…‹: {info['area_type']}")
    print(f"  ä½æ‰€: {info['address']}")
    print(f"  é›»è©±: {info['phone']}")
    
    # æœŸå¾…å€¤ã¨æ¯”è¼ƒ
    print("\nâœ… æœŸå¾…ã•ã‚Œã‚‹çµæœ:")
    print("  åº—èˆ—å: Bar Aisleï¼ˆãƒãƒ¼ ã‚¢ã‚¤ãƒ«ï¼‰")
    print("  åœ°åŸŸãƒ»æ¥­æ…‹: è¥¿é™¢ ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼")
    print("  ä½æ‰€: äº¬éƒ½åºœäº¬éƒ½å¸‚å³äº¬åŒºè¥¿é™¢é«˜å±±å¯ºç”º12-5 ã‚¸ãƒ§ã‚¤ãƒ³è¥¿é™¢ãƒ“ãƒ«7F")
    print("  é›»è©±: 075-316-0022")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    if not info["name"]:
        soup = BeautifulSoup(html, "html.parser")
        print("\nâš ï¸ ãƒ‡ãƒãƒƒã‚°: <h1>ã‚¿ã‚°:")
        for h1 in soup.find_all("h1"):
            print(f"  - {h1.get_text(strip=True)[:100]}")
    
    if not info["address"]:
        print("\nâš ï¸ ãƒ‡ãƒãƒƒã‚°: ä½æ‰€å€™è£œ:")
        for m in re.finditer(r'(äº¬éƒ½åºœ[^\n<]{10,80})', html):
            print(f"  - {m.group(1)[:80]}")
    
    if not info["phone"]:
        print("\nâš ï¸ ãƒ‡ãƒãƒƒã‚°: é›»è©±ç•ªå·å€™è£œ:")
        for m in re.finditer(r'(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})', html):
            print(f"  - {m.group(1)}")

except Exception as e:
    print(f"âœ— ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()

print("\nâœ¨ å®Œäº†")
