"""
ãƒã‚±ãƒ‘ãƒ©ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import sys
import re
import time
import urllib.request
import urllib.parse

# Windows UTF-8å‡ºåŠ›è¨­å®š
if sys.platform == "win32":
    import io
    for name in ("stdout", "stderr"):
        stream = getattr(sys, name)
        if hasattr(stream, "buffer"):
            setattr(sys, name, io.TextIOWrapper(stream.buffer, encoding="utf-8", errors="replace", line_buffering=True))

try:
    from bs4 import BeautifulSoup
    print("âœ“ BeautifulSoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
except ImportError:
    print("âœ— BeautifulSoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    sys.exit(1)

# ãƒ†ã‚¹ãƒˆURL
SHOP_URL = "https://www.pokepara.jp/kyoto/m325/a381/shop22609/"
AREA_URL = "https://www.pokepara.jp/kyoto/m325/a381/"

def fetch(url, timeout=20):
    """URLã‹ã‚‰HTMLã‚’å–å¾—"""
    print(f"ğŸ“¡ å–å¾—ä¸­: {url[:80]}...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": "https://www.pokepara.jp/",
    }
    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req, timeout=timeout) as res:
            html = res.read()
        for enc in ("utf-8", "shift_jis", "cp932"):
            try:
                return html.decode(enc)
            except:
                continue
        return html.decode("utf-8", errors="replace")
    except urllib.error.HTTPError as e:
        print(f"  âœ— HTTPError: {e.code} - {e.reason}")
        raise

def extract_shop_urls(html):
    """ä¸€è¦§HTMLã‹ã‚‰åº—èˆ—è©³ç´°URLã‚’æŠ½å‡º"""
    links = []
    soup = BeautifulSoup(html, "html.parser")
    
    print("\nğŸ” åº—èˆ—ãƒªãƒ³ã‚¯æŠ½å‡ºä¸­...")
    for a in soup.find_all("a", href=True):
        href = a.get("href", "").strip()
        # /shopæ•°å­—/ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
        if "/shop" in href and re.search(r'/shop\d+/?$', href):
            if not href.startswith("http"):
                href = f"https://www.pokepara.jp{href}"
            if href not in links:
                links.append(href)
                print(f"  âœ“ {href}")
    
    return links

def parse_shop_detail(html):
    """åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    result = {"name": "", "area_type": "", "address": "", "phone": ""}
    
    soup = BeautifulSoup(html, "html.parser")
    
    # åº—èˆ—åã‚’æŠ½å‡ºï¼ˆ<h1>ã‚¿ã‚°ã‹ã‚‰ã€ä½™åˆ†ãªéƒ¨åˆ†ã‚’é™¤å»ï¼‰
    h1 = soup.find("h1")
    if h1:
        name = h1.get_text(strip=True)
        # ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+/(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼)[^\n]*$', '', name)
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+$', '', name)
        result["name"] = name.strip()
    
    # åœ°åŸŸãƒ»æ¥­æ…‹ã‚’æŠ½å‡ºï¼ˆãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆã‹ã‚‰ï¼‰
    breadcrumb = soup.find("div", class_=re.compile(r"breadcrumb", re.I))
    if breadcrumb:
        text = breadcrumb.get_text(strip=True)
        parts = [p.strip() for p in text.split('>')]
        if len(parts) >= 2:
            area = parts[-2] if len(parts) >= 2 else ""
            genre = parts[-1] if parts[-1] in ["ã‚­ãƒ£ãƒã‚¯ãƒ©", "ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼", "ãƒ©ã‚¦ãƒ³ã‚¸", "ã‚¹ãƒŠãƒƒã‚¯", "ã‚¯ãƒ©ãƒ–", "ãƒ‘ãƒ–"] else ""
            if area and genre:
                result["area_type"] = f"{area} {genre}"
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: HTMLã‹ã‚‰ç›´æ¥æŠ½å‡º
    if not result["area_type"]:
        patterns = [
            r'(ç¥‡åœ’|æœ¨å±‹ç”º|å…ˆæ–—ç”º|æ²³åŸç”º|å››æ¡|ä¸‰æ¡|çƒä¸¸|äº¬éƒ½é§…|äºŒæ¡|è¥¿é™¢|è¥¿äº¬æ¥µ)\s*(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼|ãƒ©ã‚¦ãƒ³ã‚¸|ã‚¹ãƒŠãƒƒã‚¯|ã‚¯ãƒ©ãƒ–|ãƒ‘ãƒ–)',
            r'([^\n/]{2,10}ã‚¨ãƒªã‚¢[ã®ã«]*)\s*(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼|ãƒ©ã‚¦ãƒ³ã‚¸|ã‚¹ãƒŠãƒƒã‚¯|ã‚¯ãƒ©ãƒ–|ãƒ‘ãƒ–)',
        ]
        for pattern in patterns:
            m = re.search(pattern, html)
            if m:
                result["area_type"] = f"{m.group(1).strip()} {m.group(2)}"
                break
    
    # ä½æ‰€ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    addr_patterns = [
        r'ä½æ‰€[ï¼š:]\s*<[^>]+>([^<]+)<',
        r'ä½æ‰€[ï¼š:]\s*([^\n<]{10,150})',
        r'(äº¬éƒ½åºœäº¬éƒ½å¸‚[^\n<"]{10,150})',
        r'(äº¬éƒ½åºœ[^\n<"]{10,150})',
        r'ã€’\d{3}-\d{4}\s*([^\n<]{10,150})',
    ]
    for pattern in addr_patterns:
        m = re.search(pattern, html)
        if m:
            addr = m.group(1)
            addr = re.sub(r'<[^>]+>', '', addr)
            addr = re.sub(r'"\s*/>', '', addr)
            addr = re.sub(r'"[^"]*$', '', addr)
            addr = re.sub(r'ä½æ‰€[ï¼š:]', '', addr)
            addr = addr.strip()
            if len(addr) > 10 and ('äº¬éƒ½' in addr or 'çœŒ' in addr or 'éƒ½' in addr or 'åºœ' in addr):
                result["address"] = addr
                break
    
    # é›»è©±ç•ªå·ã‚’æŠ½å‡ºï¼ˆå„ªå…ˆé †ä½ã‚’è€ƒæ…®ï¼‰
    phone_patterns = [
        r'(?:tel|TEL|é›»è©±)[ï¼š:]\s*(0\d{1,4}[-]\d{1,4}[-]\d{4})',
        r'(?<![0-9])(0\d{1,4}[-]\d{1,4}[-]\d{4})(?![0-9])',
        r'(0\d{1,4}\s+\d{1,4}\s+\d{4})',
    ]
    for pattern in phone_patterns:
        m = re.search(pattern, html, re.I)
        if m:
            phone = m.group(1).replace(" ", "").replace("ã€€", "")
            phone_digits = re.sub(r'[^0-9]', '', phone)
            if 10 <= len(phone_digits) <= 11 and phone_digits.startswith('0'):
                result["phone"] = phone
                break
    
    return result

def clean_text(s):
    """ã‚¼ãƒ­å¹…æ–‡å­—ã‚’é™¤å»"""
    if not s:
        return ""
    zw = "\u200b\u200c\u200d\ufeff"
    return "".join(c for c in str(s).strip() if c not in zw)

def main():
    print("=" * 70)
    print("ğŸ° ãƒã‚±ãƒ‘ãƒ©ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    print()
    
    # ãƒ†ã‚¹ãƒˆ1: åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
    print("ğŸ“„ ãƒ†ã‚¹ãƒˆ1: åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹")
    print(f"URL: {SHOP_URL}")
    print()
    
    try:
        html = fetch(SHOP_URL)
        print(f"  âœ“ HTMLå–å¾—å®Œäº†: {len(html):,}æ–‡å­—")
        
        info = parse_shop_detail(html)
        
        print("\nğŸ“‹ æŠ½å‡ºçµæœ:")
        print(f"  åº—èˆ—å: {clean_text(info['name'])}")
        print(f"  åœ°åŸŸãƒ»æ¥­æ…‹: {clean_text(info['area_type'])}")
        print(f"  ä½æ‰€: {clean_text(info['address'])}")
        print(f"  é›»è©±: {clean_text(info['phone'])}")
        
        if not info["name"]:
            print("\nâš ï¸ åº—èˆ—åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print("\nHTMLå†…ã®<h1>ã‚¿ã‚°:")
            soup = BeautifulSoup(html, "html.parser")
            for h1 in soup.find_all("h1"):
                print(f"  - {h1.get_text(strip=True)}")
            
            print("\nHTMLå†…ã®ä½æ‰€ã‚‰ã—ãæ–‡å­—åˆ—:")
            for m in re.finditer(r'(äº¬éƒ½åºœ[^\n<]{10,50})', html):
                print(f"  - {m.group(1)}")
        
        # ãƒ†ã‚¹ãƒˆ2: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º
        print("\n" + "=" * 70)
        print("ğŸ“„ ãƒ†ã‚¹ãƒˆ2: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLæŠ½å‡º")
        print(f"URL: {AREA_URL}")
        print()
        
        time.sleep(1)
        list_html = fetch(AREA_URL)
        print(f"  âœ“ HTMLå–å¾—å®Œäº†: {len(list_html):,}æ–‡å­—")
        
        shop_urls = extract_shop_urls(list_html)
        print(f"\n  âœ“ åº—èˆ—URLç™ºè¦‹: {len(shop_urls)}ä»¶")
        
        if shop_urls:
            print("\nğŸ‰ æˆåŠŸï¼ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡ºã§ãã¾ã—ãŸ")
            print("\næœ€åˆã®5ä»¶:")
            for url in shop_urls[:5]:
                print(f"  - {url}")
        else:
            print("\nâš ï¸ åº—èˆ—URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ¨ å®Œäº†")

if __name__ == "__main__":
    main()
