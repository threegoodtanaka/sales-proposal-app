"""
ã‚µãƒ³ãƒˆãƒªãƒ¼ãƒãƒ¼ãƒŠãƒ“ç°¡æ˜“ãƒ†ã‚¹ãƒˆ - æ¤œç´¢èªã‚’ä½¿ã‚ãªã„åº—èˆ—ãƒšãƒ¼ã‚¸ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
"""
import sys
import re
import time
import urllib.request

if sys.platform == "win32":
    import io
    for name in ("stdout", "stderr"):
        stream = getattr(sys, name)
        if hasattr(stream, "buffer"):
            setattr(sys, name, io.TextIOWrapper(stream.buffer, encoding="utf-8", errors="replace", line_buffering=True))

try:
    from bs4 import BeautifulSoup
    print("âœ“ BeautifulSoup4 OK")
except ImportError:
    print("âœ— BeautifulSoup4 ãŒå¿…è¦ã§ã™")
    sys.exit(1)

# ç›´æ¥åº—èˆ—ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆä¾‹: ãƒ›ãƒ³ã‚­ãƒ¼ãƒˆãƒ³ã‚¯ï¼‰
TEST_URL = "https://bar-navi.suntory.co.jp/shop/0757018015/"

def fetch(url):
    """URLã‹ã‚‰HTMLã‚’å–å¾—"""
    print(f"ğŸ“¡ å–å¾—ä¸­: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en;q=0.9",
        "Referer": "https://bar-navi.suntory.co.jp/",
    }
    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, timeout=15) as res:
        html = res.read()
    return html.decode("utf-8", errors="replace")

def parse_shop(html):
    """åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
    result = {"name": "", "address": "", "phone": ""}
    
    # åº—å
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.I)
    if m:
        result["name"] = m.group(1).strip()
    
    # ä½æ‰€ - è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³è©¦è¡Œ
    patterns = [
        r'ä½æ‰€[ï¼š:]\s*<[^>]+>([^<]+)<',
        r'(äº¬éƒ½åºœ[^\n<]{10,100})',
        r'ã€’\d{3}-\d{4}[^\n<]{10,100}',
    ]
    for p in patterns:
        m = re.search(p, html)
        if m:
            addr = m.group(1)
            addr = re.sub(r'<[^>]+>', '', addr)
            addr = addr.strip()
            if len(addr) > 10:
                result["address"] = addr
                break
    
    # é›»è©±ç•ªå·
    m = re.search(r'(\d{2,4}[-â€âˆ’ãƒ¼]\d{3,4}[-â€âˆ’ãƒ¼]\d{4})', html)
    if m:
        result["phone"] = m.group(1).replace("âˆ’", "-").replace("â€", "-").replace("ãƒ¼", "-")
    
    return result

print("=" * 70)
print("ğŸº ã‚µãƒ³ãƒˆãƒªãƒ¼ãƒãƒ¼ãƒŠãƒ“ åº—èˆ—ãƒšãƒ¼ã‚¸ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ")
print("=" * 70)

try:
    html = fetch(TEST_URL)
    print(f"âœ“ HTMLå–å¾—: {len(html):,}æ–‡å­—\n")
    
    info = parse_shop(html)
    
    print("ğŸ“‹ æŠ½å‡ºçµæœ:")
    print(f"  åº—èˆ—å: {info['name']}")
    print(f"  ä½æ‰€: {info['address']}")
    print(f"  é›»è©±: {info['phone']}")
    
    if not info["name"]:
        print("\nâš ï¸ åº—èˆ—åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        print("\nHTMLå†…ã®<h1>ã‚¿ã‚°:")
        for m in re.finditer(r'<h1[^>]*>([^<]+)</h1>', html, re.I):
            print(f"  - {m.group(1)}")
        
        print("\nHTMLå†…ã®ä½æ‰€ã‚‰ã—ãæ–‡å­—åˆ—:")
        for m in re.finditer(r'(äº¬éƒ½åºœ[^\n<]{10,50})', html):
            print(f"  - {m.group(1)}")
    
    print("\nâœ¨ å®Œäº†")

except urllib.error.HTTPError as e:
    print(f"âœ— HTTPError: {e.code} - {e.reason}")
    print(f"åº—èˆ—ãƒšãƒ¼ã‚¸ã‚‚403ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™")
except Exception as e:
    print(f"âœ— ã‚¨ãƒ©ãƒ¼: {e}")
