"""
ãƒã‚±ãƒ‘ãƒ©ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ã‚¹ãƒˆ
"""
import sys
import re
import urllib.request

if sys.platform == "win32":
    import io
    for name in ("stdout", "stderr"):
        stream = getattr(sys, name)
        if hasattr(stream, "buffer"):
            setattr(sys, name, io.TextIOWrapper(stream.buffer, encoding="utf-8", errors="replace", line_buffering=True))

try:
    from bs4 import BeautifulSoup
    print("âœ“ BeautifulSoup4 OK")
except ImportError:
    print("âœ— BeautifulSoup4 ãŒå¿…è¦ã§ã™")
    sys.exit(1)

# ãƒ†ã‚¹ãƒˆURL: ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
TEST_URLS = [
    "https://www.pokepara.jp/kyoto/m325/a381/shop22609/",  # ãƒ‘ã‚¿ãƒ¼ãƒ³1
    "https://www.pokepara.jp/kyoto/m325/a384/shop12005/",  # ãƒ‘ã‚¿ãƒ¼ãƒ³2ï¼ˆæŒ‡å®šã•ã‚ŒãŸURLï¼‰
]

def fetch(url):
    """URLã‹ã‚‰HTMLã‚’å–å¾—"""
    print(f"ğŸ“¡ å–å¾—ä¸­: {url}")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en;q=0.9",
        "Referer": "https://www.pokepara.jp/",
    }
    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, timeout=15) as res:
        html = res.read()
    for enc in ("utf-8", "shift_jis", "cp932"):
        try:
            return html.decode(enc)
        except:
            continue
    return html.decode("utf-8", errors="replace")

def parse_shop_v1(html):
    """ç¾åœ¨ã®å®Ÿè£…ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼‰"""
    result = {"name": "", "area_type": "", "address": "", "phone": ""}
    
    # åº—èˆ—å
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.I)
    if m:
        name = m.group(1).strip()
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+$', '', name)
        result["name"] = name
    
    # åœ°åŸŸãƒ»æ¥­æ…‹
    m = re.search(r'([^\n/]{2,15})\s*(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼|ãƒ©ã‚¦ãƒ³ã‚¸|ã‚¹ãƒŠãƒƒã‚¯|ã‚¯ãƒ©ãƒ–|ãƒ‘ãƒ–)', html)
    if m:
        result["area_type"] = f"{m.group(1).strip()} {m.group(2)}"
    
    # ä½æ‰€
    patterns = [
        r'(äº¬éƒ½åºœäº¬éƒ½å¸‚[^\n<"]{10,150})',
        r'(äº¬éƒ½åºœ[^\n<"]{10,150})',
    ]
    for pattern in patterns:
        m = re.search(pattern, html)
        if m:
            addr = m.group(1)
            addr = re.sub(r'<[^>]+>', '', addr)
            addr = re.sub(r'"\s*/>', '', addr)
            addr = re.sub(r'"[^"]*$', '', addr)
            addr = addr.strip()
            if len(addr) > 10 and ('çœŒ' in addr or 'éƒ½' in addr or 'åºœ' in addr):
                result["address"] = addr
                break
    
    # é›»è©±ç•ªå·
    m = re.search(r'(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})', html)
    if m:
        result["phone"] = m.group(1).replace(" ", "").replace("ã€€", "")
    
    return result

def parse_shop_v2(html, soup):
    """å¼·åŒ–ç‰ˆ: ã‚ˆã‚Šå¤šãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œ"""
    result = {"name": "", "area_type": "", "address": "", "phone": ""}
    
    # åº—èˆ—å: è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    # 1. <h1>ã‚¿ã‚°
    h1 = soup.find("h1")
    if h1:
        name = h1.get_text(strip=True)
        # ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤å»
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+/(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼)[^\n]*$', '', name)
        name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+$', '', name)
        result["name"] = name.strip()
    
    # 2. titleã‚¿ã‚°ã‚„metaã‚¿ã‚°ã‹ã‚‰ã‚‚è©¦è¡Œ
    if not result["name"]:
        title = soup.find("title")
        if title:
            name = title.get_text(strip=True)
            name = re.sub(r'\s*[-â€“|]\s*ãƒã‚±ãƒ‘ãƒ©.*$', '', name)
            name = re.sub(r'\s*[-â€“âˆ’]\s*[^-â€“âˆ’]+/(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼)[^\n]*$', '', name)
            result["name"] = name.strip()
    
    # åœ°åŸŸãƒ»æ¥­æ…‹: è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œ
    patterns = [
        r'([^\n/]{2,15})\s*(ã‚­ãƒ£ãƒã‚¯ãƒ©|ã‚¬ãƒ¼ãƒ«ã‚ºãƒãƒ¼|ãƒ©ã‚¦ãƒ³ã‚¸|ã‚¹ãƒŠãƒƒã‚¯|ã‚¯ãƒ©ãƒ–|ãƒ‘ãƒ–)',
        r'>(ç¥‡åœ’|æœ¨å±‹ç”º|å…ˆæ–—ç”º|æ²³åŸç”º|å››æ¡|ä¸‰æ¡|çƒä¸¸|äº¬éƒ½é§…|äºŒæ¡)[^\n<]*<',
    ]
    for pattern in patterns:
        m = re.search(pattern, html)
        if m:
            if len(m.groups()) >= 2:
                result["area_type"] = f"{m.group(1).strip()} {m.group(2)}"
            else:
                result["area_type"] = m.group(1).strip()
            break
    
    # ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆã‹ã‚‰åœ°åŸŸãƒ»æ¥­æ…‹ã‚’å–å¾—
    if not result["area_type"]:
        breadcrumb = soup.find("div", class_=re.compile(r"breadcrumb|category", re.I))
        if breadcrumb:
            text = breadcrumb.get_text(strip=True)
            parts = text.split()
            if len(parts) >= 2:
                result["area_type"] = " ".join(parts[-2:])
    
    # ä½æ‰€: ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³
    addr_patterns = [
        r'ä½æ‰€[ï¼š:]\s*<[^>]+>([^<]+)<',
        r'ä½æ‰€[ï¼š:]\s*([^\n<]{10,150})',
        r'(äº¬éƒ½åºœäº¬éƒ½å¸‚[^\n<"]{10,150})',
        r'(äº¬éƒ½åºœ[^\n<"]{10,150})',
        r'(å¤§é˜ªåºœ[^\n<"]{10,150})',
        r'(æ±äº¬éƒ½[^\n<"]{10,150})',
        r'([ä¸€-é¾¥]{2,3}çœŒ[^\n<"]{10,150})',
        r'ã€’\d{3}-\d{4}\s*([^\n<]{10,150})',
    ]
    for pattern in addr_patterns:
        m = re.search(pattern, html)
        if m:
            addr = m.group(1)
            # HTMLã‚¿ã‚°ã‚’é™¤å»
            addr = re.sub(r'<[^>]+>', '', addr)
            # ä½™åˆ†ãªæ–‡å­—ã‚’é™¤å»
            addr = re.sub(r'"\s*/>', '', addr)
            addr = re.sub(r'"[^"]*$', '', addr)
            addr = re.sub(r'ä½æ‰€[ï¼š:]', '', addr)
            addr = addr.strip()
            if len(addr) > 10 and ('çœŒ' in addr or 'éƒ½' in addr or 'åºœ' in addr or 'å¸‚' in addr):
                result["address"] = addr
                break
    
    # é›»è©±ç•ªå·: ã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³
    phone_patterns = [
        r'tel[ï¼š:]\s*(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
        r'é›»è©±[ï¼š:]\s*(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
        r'TEL[ï¼š:]\s*(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
        r'(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
    ]
    for pattern in phone_patterns:
        m = re.search(pattern, html, re.I)
        if m:
            phone = m.group(1).replace(" ", "").replace("ã€€", "")
            result["phone"] = phone
            break
    
    return result

print("=" * 70)
print("ğŸ° ãƒã‚±ãƒ‘ãƒ© è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ†ã‚¹ãƒˆ")
print("=" * 70)

for i, url in enumerate(TEST_URLS):
    print(f"\nğŸ“„ ãƒ†ã‚¹ãƒˆ {i+1}/{len(TEST_URLS)}")
    print(f"URL: {url}")
    print()
    
    try:
        html = fetch(url)
        print(f"âœ“ HTMLå–å¾—: {len(html):,}æ–‡å­—\n")
        
        soup = BeautifulSoup(html, "html.parser")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼ˆç¾åœ¨ã®å®Ÿè£…ï¼‰
        result_v1 = parse_shop_v1(html)
        print("ğŸ“‹ ç¾åœ¨ã®å®Ÿè£…ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³1ï¼‰:")
        print(f"  åº—èˆ—å: {result_v1['name']}")
        print(f"  åœ°åŸŸãƒ»æ¥­æ…‹: {result_v1['area_type']}")
        print(f"  ä½æ‰€: {result_v1['address']}")
        print(f"  é›»è©±: {result_v1['phone']}")
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        result_v2 = parse_shop_v2(html, soup)
        print("\nğŸ“‹ å¼·åŒ–ç‰ˆï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³2ï¼‰:")
        print(f"  åº—èˆ—å: {result_v2['name']}")
        print(f"  åœ°åŸŸãƒ»æ¥­æ…‹: {result_v2['area_type']}")
        print(f"  ä½æ‰€: {result_v2['address']}")
        print(f"  é›»è©±: {result_v2['phone']}")
        
        # æ¯”è¼ƒ
        print("\nğŸ” æ¯”è¼ƒ:")
        if result_v1 != result_v2:
            print("  âš ï¸ çµæœãŒç•°ãªã‚Šã¾ã™")
            if not result_v1['name'] and result_v2['name']:
                print("    â†’ å¼·åŒ–ç‰ˆã§åº—èˆ—åã‚’æŠ½å‡ºã§ãã¾ã—ãŸ")
            if not result_v1['address'] and result_v2['address']:
                print("    â†’ å¼·åŒ–ç‰ˆã§ä½æ‰€ã‚’æŠ½å‡ºã§ãã¾ã—ãŸ")
        else:
            print("  âœ“ ä¸¡æ–¹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åŒã˜çµæœã§ã™")
        
        # HTMLã®ä¸€éƒ¨ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if not result_v2['name'] or not result_v2['address']:
            print("\nâš ï¸ ä¸€éƒ¨ã®æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
            print("\nHTML ã®<h1>ã‚¿ã‚°:")
            for h1 in soup.find_all("h1"):
                print(f"  - {h1.get_text(strip=True)[:100]}")
            
            print("\nHTML å†…ã®ä½æ‰€ã‚‰ã—ãæ–‡å­—åˆ—:")
            for m in re.finditer(r'(äº¬éƒ½åºœ[^\n<]{10,50})', html):
                print(f"  - {m.group(1)[:80]}")
        
    except Exception as e:
        print(f"âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

print("\nâœ¨ å®Œäº†")
