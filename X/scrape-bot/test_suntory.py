"""
ã‚µãƒ³ãƒˆãƒªãƒ¼ãƒãƒ¼ãƒŠãƒ“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import sys
import re
import time
import urllib.request
import urllib.parse

# Windows UTF-8å‡ºåŠ›è¨­å®š
if sys.platform == "win32":
    import io
    for name in ("stdout", "stderr"):
        stream = getattr(sys, name)
        if hasattr(stream, "buffer"):
            setattr(sys, name, io.TextIOWrapper(stream.buffer, encoding="utf-8", errors="replace", line_buffering=True))

try:
    from bs4 import BeautifulSoup
    print("âœ“ BeautifulSoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
except ImportError:
    print("âœ— BeautifulSoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install beautifulsoup4")
    sys.exit(1)

# ã‚µãƒ³ãƒˆãƒªãƒ¼ãƒãƒ¼ãƒŠãƒ“URLï¼ˆäº¬éƒ½ï¼‰
URL = "https://bar-navi.suntory.co.jp/search/freeword/query___8B_9E_93s_95_7B/"

def fetch(url, timeout=20):
    """URLã‹ã‚‰HTMLã‚’å–å¾—"""
    print(f"ğŸ“¡ å–å¾—ä¸­: {url[:80]}...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://bar-navi.suntory.co.jp/",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Cache-Control": "max-age=0",
    }
    req = urllib.request.Request(url, headers=headers)
    try:
        with urllib.request.urlopen(req, timeout=timeout) as res:
            html = res.read()
        # gzipè§£å‡ãŒå¿…è¦ãªå ´åˆ
        if html[:2] == b'\x1f\x8b':
            import gzip
            html = gzip.decompress(html)
    except urllib.error.HTTPError as e:
        print(f"  âœ— HTTPError: {e.code} - {e.reason}")
        print(f"  ãƒ˜ãƒƒãƒ€ãƒ¼: {e.headers}")
        raise
    except Exception as e:
        print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    for enc in ("utf-8", "shift_jis", "cp932"):
        try:
            return html.decode(enc)
        except:
            continue
    return html.decode("utf-8", errors="replace")

def extract_shop_urls(html):
    """ä¸€è¦§HTMLã‹ã‚‰åº—èˆ—è©³ç´°URLã‚’æŠ½å‡º"""
    links = []
    soup = BeautifulSoup(html, "html.parser")
    
    print("\nğŸ” ãƒªãƒ³ã‚¯æŠ½å‡ºä¸­...")
    for a in soup.find_all("a", href=True):
        href = a.get("href", "").strip()
        if "/shop/" in href and re.search(r"/shop/\d+/?$", href):
            if not href.startswith("http"):
                href = f"https://bar-navi.suntory.co.jp{href}"
            if href not in links:
                links.append(href)
                print(f"  âœ“ {href}")
    
    return links

def parse_shop_detail(html):
    """åº—èˆ—è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
    result = {"name": "", "address": "", "phone": ""}
    
    # åº—èˆ—åã‚’æŠ½å‡ºï¼ˆ<h1>ã‚¿ã‚°ã‹ã‚‰ï¼‰
    m = re.search(r'<h1[^>]*>([^<]+)</h1>', html, re.I)
    if m:
        result["name"] = m.group(1).strip()
    
    # ä½æ‰€ã‚’æŠ½å‡ºï¼ˆéƒ½é“åºœçœŒã‹ã‚‰å§‹ã¾ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    patterns = [
        r'(äº¬éƒ½åºœ|å¤§é˜ªåºœ|æ±äº¬éƒ½|[ä¸€-é¾¥]{2,3}çœŒ)[^\n<]{10,150}',
        r'ä½æ‰€[ï¼š:]\s*([^\n<]{10,150})',
        r'ã€’\d{3}-\d{4}\s*([^\n<]{10,150})',
    ]
    for pattern in patterns:
        m = re.search(pattern, html)
        if m:
            addr = m.group(1) if len(m.groups()) > 0 else m.group(0)
            addr = re.sub(r'<[^>]+>', '', addr)
            addr = re.sub(r'\s+', '', addr)
            if len(addr) > 5 and 'çœŒ' in addr or 'éƒ½' in addr or 'åºœ' in addr:
                result["address"] = addr
                break
    
    # é›»è©±ç•ªå·ã‚’æŠ½å‡º
    patterns = [
        r'tel[ï¼š:]\s*(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
        r'é›»è©±[ï¼š:]\s*(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
        r'(0\d{1,4}[-\s]?\d{1,4}[-\s]?\d{4})',
    ]
    for pattern in patterns:
        m = re.search(pattern, html, re.I)
        if m:
            phone = m.group(1).replace(" ", "").replace("ã€€", "")
            result["phone"] = phone
            break
    
    return result

def clean_text(s):
    """ã‚¼ãƒ­å¹…æ–‡å­—ã‚’é™¤å»"""
    if not s:
        return ""
    zw = "\u200b\u200c\u200d\ufeff"
    return "".join(c for c in str(s).strip() if c not in zw)

def main():
    print("=" * 70)
    print("ğŸº ã‚µãƒ³ãƒˆãƒªãƒ¼ãƒãƒ¼ãƒŠãƒ“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    print()
    
    # ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—
    print("ğŸ“„ ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    try:
        list_html = fetch(URL)
        print(f"  âœ“ HTMLå–å¾—å®Œäº†: {len(list_html):,}æ–‡å­—")
    except Exception as e:
        print(f"\nâŒ ä¸€è¦§ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        print("\nğŸ’¡ å¯¾å‡¦æ³•:")
        print("1. ãƒ–ãƒ©ã‚¦ã‚¶ã§URLã‚’é–‹ã„ã¦ã€ãƒšãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‹ç¢ºèª")
        print("2. User-Agentã‚„Refererã®èª¿æ•´ãŒå¿…è¦ãªå¯èƒ½æ€§")
        return
    
    # HTMLã®å…ˆé ­ã‚’ç¢ºèª
    print("\nğŸ“Š HTML ã®å…ˆé ­500æ–‡å­—:")
    print(list_html[:500])
    
    # è©³ç´°URLæŠ½å‡º
    shop_urls = extract_shop_urls(list_html)
    print(f"\n  âœ“ åº—èˆ—URLæŠ½å‡º: {len(shop_urls)}ä»¶")
    
    if not shop_urls:
        print("\nâŒ åº—èˆ—URLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        print("\nğŸ“Š HTMLå†…å®¹ã‚’ç¢ºèª:")
        print(list_html[:2000])
        return
    
    # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±å–å¾—ï¼ˆæœ€åˆã®3ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆï¼‰
    print(f"\nğŸ” è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±å–å¾—ä¸­... (æœ€å¤§3ä»¶)")
    results = []
    
    for i, url in enumerate(shop_urls[:3]):
        print(f"\n  [{i+1}/{min(len(shop_urls), 3)}] {url}")
        time.sleep(1)
        try:
            html = fetch(url)
            info = parse_shop_detail(html)
            if info["name"]:
                results.append(info)
                print(f"    âœ“ {clean_text(info['name'])}")
                print(f"      ä½æ‰€: {clean_text(info['address'])[:50]}...")
                print(f"      TEL: {clean_text(info['phone'])}")
            else:
                print(f"    âš ï¸ æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—")
        except Exception as e:
            print(f"    âœ— ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 70)
    print(f"âœ… å–å¾—å®Œäº†: {len(results)}ä»¶")
    print("=" * 70)
    
    if results:
        print("\nğŸ“‹ CSVå‡ºåŠ›:")
        print("åº—èˆ—å,ä½æ‰€,é›»è©±ç•ªå·")
        for r in results:
            name = clean_text(r["name"]).replace(",", "")
            addr = clean_text(r["address"]).replace(",", " ")
            phone = clean_text(r["phone"])
            print(f"{name},{addr},{phone}")
    
    print("\nâœ¨ å®Œäº†")

if __name__ == "__main__":
    main()
