"""
é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆã‚¿ãƒ¼ãƒŸãƒŠãƒ«å®Ÿè¡Œç”¨ï¼‰
"""
import sys
import json
import time
import re
import urllib.request
import urllib.parse

# Windows UTF-8å‡ºåŠ›è¨­å®š
if sys.platform == "win32":
    import io
    for name in ("stdout", "stderr"):
        stream = getattr(sys, name)
        if hasattr(stream, "buffer"):
            setattr(sys, name, io.TextIOWrapper(stream.buffer, encoding="utf-8", errors="replace", line_buffering=True))

try:
    from bs4 import BeautifulSoup
    print("âœ“ BeautifulSoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã™")
except ImportError:
    print("âœ— BeautifulSoup4 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install beautifulsoup4")
    sys.exit(1)

# é£Ÿã¹ãƒ­ã‚°URLï¼ˆå—ä¸¹å¸‚ãƒ†ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆï¼‰
URL = "https://tabelog.com/kyoto/C26213/rstLst/cond10-04-00/?vs=1&sa=%E5%8D%97%E4%B8%B9%E5%B8%82&sk=%25E3%2583%2586%25E3%2582%25A4%25E3%2582%25AF%25E3%2582%25A2%25E3%2582%25A6%25E3%2583%2588&lid=hd_search1&ChkTakeout=1&cat_sk=%E3%83%86%E3%82%A4%E3%82%AF%E3%82%A2%E3%82%A6%E3%83%88"

def fetch(url, timeout=20):
    """URLã‹ã‚‰HTMLã‚’å–å¾—"""
    print(f"ğŸ“¡ å–å¾—ä¸­: {url[:80]}...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Referer": "https://tabelog.com/",
    }
    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req, timeout=timeout) as res:
        html = res.read()
    for enc in ("utf-8", "cp932", "shift_jis"):
        try:
            return html.decode(enc)
        except:
            continue
    return html.decode("utf-8", errors="replace")

def extract_detail_urls_from_jsonld(html):
    """JSON-LD ItemList ã‹ã‚‰è©³ç´°URLã‚’æŠ½å‡º"""
    links = []
    for m in re.finditer(r'<script[^>]*type\s*=\s*["\']application/ld\+json["\'][^>]*>([^<]+)</script>', html, re.I | re.S):
        try:
            data = json.loads(m.group(1).strip())
            if isinstance(data, dict) and data.get("@type") == "ItemList":
                print(f"  âœ“ JSON-LD ItemList ç™ºè¦‹: {len(data.get('itemListElement', []))}é …ç›®")
                for item in data.get("itemListElement") or []:
                    if isinstance(item, dict) and item.get("url"):
                        u = item["url"].strip()
                        if u and "tabelog.com" in u and "rstlst" not in u.lower():
                            links.append(u)
        except:
            pass
    
    # JSON-LDãŒãªã„å ´åˆã¯data-detail-urlã‹ã‚‰æŠ½å‡º
    if not links:
        print("  âš  JSON-LD ItemList ãªã—ã€data-detail-url ã‹ã‚‰æŠ½å‡º")
        seen = set()
        for m in re.finditer(r'data-detail-url\s*=\s*["\'](https?://[^"\']*tabelog\.com/[^"\']*/\d{6,}/?)["\']', html, re.I):
            u = m.group(1).rstrip("/") + "/"
            if u not in seen and "rstlst" not in u.lower():
                seen.add(u)
                links.append(u)
        print(f"  âœ“ data-detail-url ã‹ã‚‰ {len(links)}ä»¶æŠ½å‡º")
    
    return links

def parse_detail(html):
    """è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—åãƒ»é›»è©±ç•ªå·ãƒ»ä½æ‰€ã‚’æŠ½å‡º"""
    result = {"name": "", "phone": "", "address": ""}
    
    # JSON-LD Restaurantã‹ã‚‰æŠ½å‡º
    for m in re.finditer(r'<script[^>]*type\s*=\s*["\']application/ld\+json["\'][^>]*>([^<]+)</script>', html, re.I | re.S):
        try:
            data = json.loads(m.group(1).strip())
            if isinstance(data, dict) and data.get("@type") == "Restaurant":
                result["name"] = data.get("name", "").strip()
                result["phone"] = data.get("telephone", "").strip()
                addr = data.get("address", {})
                if isinstance(addr, dict):
                    result["address"] = addr.get("streetAddress", "").strip()
                break
        except:
            pass
    
    # é›»è©±ç•ªå·ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not result["phone"]:
        m = re.search(r'<strong[^>]*>\s*(\d{2,5}-\d{1,4}-\d{4})\s*</strong>', html)
        if m:
            result["phone"] = m.group(1)
    
    return result

def clean_text(s):
    """ã‚¼ãƒ­å¹…æ–‡å­—ã‚’é™¤å»"""
    if not s:
        return ""
    zw = "\u200b\u200c\u200d\ufeff"
    return "".join(c for c in str(s).strip() if c not in zw)

def main():
    print("=" * 70)
    print("ğŸ´ é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("=" * 70)
    print()
    
    # 1ãƒšãƒ¼ã‚¸ç›®å–å¾—
    print("ğŸ“„ 1ãƒšãƒ¼ã‚¸ç›®ã‚’å–å¾—ä¸­...")
    list_html = fetch(URL)
    print(f"  âœ“ HTMLå–å¾—å®Œäº†: {len(list_html):,}æ–‡å­—")
    
    # è©³ç´°URLæŠ½å‡º
    detail_urls = extract_detail_urls_from_jsonld(list_html)
    print(f"  âœ“ è©³ç´°URLæŠ½å‡º: {len(detail_urls)}ä»¶")
    
    if not detail_urls:
        print("\nâŒ è©³ç´°URLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        print("\nğŸ“Š HTML ã®å…ˆé ­500æ–‡å­—:")
        print(list_html[:500])
        return
    
    # 2ãƒšãƒ¼ã‚¸ç›®ã‚‚å–å¾—ï¼ˆ23ä»¶å¯¾å¿œï¼‰
    if len(detail_urls) == 20:
        print("\nğŸ“„ 2ãƒšãƒ¼ã‚¸ç›®ã‚’å–å¾—ä¸­...")
        time.sleep(1)
        parsed = urllib.parse.urlparse(URL)
        path = parsed.path.rstrip("/")
        if "/2/" not in path:
            base = re.sub(r"/\d+/?(?:\?.*)?$", "", path)
            next_path = base + "/2/" + ("?" + parsed.query if parsed.query else "")
            next_url = urllib.parse.urlunparse((parsed.scheme, parsed.netloc, next_path, "", "", ""))
            
            try:
                list2 = fetch(next_url)
                urls2 = extract_detail_urls_from_jsonld(list2)
                for u in urls2:
                    if u not in detail_urls:
                        detail_urls.append(u)
                print(f"  âœ“ åˆè¨ˆURLæ•°: {len(detail_urls)}ä»¶")
            except Exception as e:
                print(f"  âš  2ãƒšãƒ¼ã‚¸ç›®å–å¾—å¤±æ•—: {e}")
    
    # è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±å–å¾—
    print(f"\nğŸ” è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±å–å¾—ä¸­... (æœ€å¤§{min(len(detail_urls), 5)}ä»¶)")
    results = []
    
    for i, url in enumerate(detail_urls[:5]):  # æœ€åˆã®5ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆ
        print(f"\n  [{i+1}/{min(len(detail_urls), 5)}] {url}")
        time.sleep(0.6)
        try:
            html = fetch(url)
            info = parse_detail(html)
            if info["name"] or info["phone"]:
                results.append(info)
                print(f"    âœ“ {clean_text(info['name'])}")
                print(f"      TEL: {clean_text(info['phone'])}")
                print(f"      ä½æ‰€: {clean_text(info['address'])[:40]}...")
        except Exception as e:
            print(f"    âœ— ã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 70)
    print(f"âœ… å–å¾—å®Œäº†: {len(results)}ä»¶")
    print("=" * 70)
    
    if results:
        print("\nğŸ“‹ CSVå‡ºåŠ›:")
        print("åº—å,é›»è©±ç•ªå·,ä½æ‰€")
        for r in results:
            name = clean_text(r["name"]).replace(",", "")
            phone = clean_text(r["phone"])
            addr = clean_text(r["address"]).replace(",", " ")
            print(f"{name},{phone},{addr}")
    
    print("\nâœ¨ å®Œäº†")

if __name__ == "__main__":
    main()
